{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Using dynamic programming we can solve the RL problem when all environment dynamics are known and we have a finite state set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any environment that our agent can explore needs to be described using a subclass of `Environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def get_spaces(self):\n",
    "        \"\"\"\n",
    "        Return the set of possible states\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\"\n",
    "        Returns the actions that can be taken from the given state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Executes the action and changes the state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def reward(self, action, state, new_state):\n",
    "        \"\"\"\n",
    "        Returns the reward for the given state change\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DPAgent` can find a perfect policy given an `Environment`. It works using value iteration. Initially, a random policy is generated. After that, each time `agent.learn()` is executed, we perform:\n",
    "\n",
    "* policy evaluation: The value for each state is updated, by using the values of neighbouring states\n",
    "* policy improvement: For each state we choose the action that maximizes the expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from operator import itemgetter\n",
    "\n",
    "class DPAgent:\n",
    "    def __init__(self, env, discount_factor=1):\n",
    "        self.env = env\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        states = self.states = env.get_spaces()\n",
    "        self.values = { state: 0 for state in states }\n",
    "        self.policy = { state: set(sample(env.get_possible_actions(state), 1)) for state in states }\n",
    "        \n",
    "    def learn(self):\n",
    "        self._evaluate_policy()\n",
    "        self._improve_policy()\n",
    "        \n",
    "    def _evaluate_policy(self):\n",
    "        values = self.values.copy()\n",
    "        \n",
    "        for state in self.states:\n",
    "            value = self.values[state]\n",
    "            actions = self.policy[state]\n",
    "            change = 0.\n",
    "            \n",
    "            for action in actions:\n",
    "                new_state = self.env.execute_action(state, action)\n",
    "                next_value = self.values[new_state]\n",
    "                reward = self.env.reward(action, state, new_state)\n",
    "                \n",
    "                change += reward + self.discount_factor * next_value\n",
    "            \n",
    "            values[state] = change / len(actions)\n",
    "\n",
    "        self.values = values\n",
    "    \n",
    "    def _improve_policy(self):\n",
    "        for state in self.states:\n",
    "            action = self.policy[state]\n",
    "            possible_actions = {}\n",
    "\n",
    "            for possible_action in self.env.get_possible_actions(state):\n",
    "                new_state = self.env.execute_action(state, possible_action)\n",
    "                \n",
    "                value = self.values[new_state]\n",
    "                reward = self.env.reward(action, state, new_state)\n",
    "                \n",
    "                possible_actions[possible_action] = reward + self.discount_factor * value\n",
    "            \n",
    "            max_value = max(possible_actions.values())            \n",
    "            self.policy[state] = set([action for action, value in possible_actions.items() if value == max_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridWorld\n",
    "\n",
    "GridWorld is on of the standard examples for dynamic programming. Our agent can move in a 2d matrix. The rewards are defined in a matrix of the same size. There environment is totally deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld(Environment):\n",
    "    def __init__(self, reward_matrix):\n",
    "        self.reward_matrix = reward_matrix\n",
    "        \n",
    "        n, m = reward_matrix.shape\n",
    "        self.states = list(product(range(n), range(m)))\n",
    "        \n",
    "        self.max_down = n - 1\n",
    "        self.max_right = m - 1\n",
    "        \n",
    "        self._init_actions()\n",
    "        \n",
    "    def _init_actions(self):\n",
    "        self.UP = \"U\"\n",
    "        self.DOWN = \"D\"\n",
    "        self.LEFT = \"L\"\n",
    "        self.RIGHT = \"R\"\n",
    "        \n",
    "        self.actions = [self.UP, self.DOWN, self.LEFT, self.RIGHT]\n",
    "        \n",
    "    def get_spaces(self):\n",
    "        return self.states\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        i, j = state\n",
    "        \n",
    "        actions = []\n",
    "        \n",
    "        if i > 0:\n",
    "            actions.append(self.UP)\n",
    "            \n",
    "        if i < self.max_down:\n",
    "            actions.append(self.DOWN)\n",
    "        \n",
    "        if j > 0:\n",
    "            actions.append(self.LEFT)\n",
    "        \n",
    "        if j < self.max_right:\n",
    "            actions.append(self.RIGHT)\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        i, j = state\n",
    "        \n",
    "        if action == self.UP:\n",
    "            i -= 1\n",
    "            \n",
    "        if action == self.DOWN:\n",
    "            i += 1\n",
    "            \n",
    "        if action == self.LEFT:\n",
    "            j -= 1\n",
    "            \n",
    "        if action == self.RIGHT:\n",
    "            j += 1\n",
    "            \n",
    "        i = max(0, min(i, self.max_down))\n",
    "        j = max(0, min(j, self.max_right))\n",
    "        \n",
    "        return i, j\n",
    "    \n",
    "    def reward(self, action, state, new_state):\n",
    "        return self.reward_matrix[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions for visualizing the policy/values/rewards of GridWorld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_policy(agent, n, m):\n",
    "    lookup = {\n",
    "        \"U\": \"^\",\n",
    "        \"D\": \"v\",\n",
    "        \"L\": \"<\",\n",
    "        \"R\": \">\"\n",
    "    }\n",
    "    \n",
    "    lookup = {\n",
    "        \"U\": u\"↑\",\n",
    "        \"D\": u\"↓\",\n",
    "        \"L\": u\"←\",\n",
    "        \"R\": u\"→\"\n",
    "    }\n",
    "    \n",
    "    vals = sorted(agent.policy.items())\n",
    "    actions = [\"\".join([lookup[act] for act in val]) for _, val in vals]\n",
    "    \n",
    "    print \"Policy\"\n",
    "    draw_matrix(np.array(actions).reshape((n, m)))\n",
    "    print\n",
    "\n",
    "def draw_values(agent, n, m):\n",
    "    vals = sorted(agent.values.items())\n",
    "    vals = [\"%.2f\" % val for _, val in vals]\n",
    "    \n",
    "    print \"Values\"\n",
    "    draw_matrix(np.array(vals).reshape((n, m)))\n",
    "    print\n",
    "    \n",
    "from math import ceil, floor\n",
    "\n",
    "def lrjust(s, size):\n",
    "    missing = size - len(s)\n",
    "    \n",
    "    if missing > 0:\n",
    "        left = int(ceil(missing / 2.))\n",
    "        right = int(floor(missing / 2.))\n",
    "        s = \" \" * left + s + \" \" * right\n",
    "    \n",
    "    return s\n",
    "\n",
    "def draw_matrix(data):    \n",
    "    result = \"\"\n",
    "    rows = []\n",
    "    \n",
    "    for row in data:\n",
    "        row_result = []\n",
    "        \n",
    "        for cell in row:\n",
    "            row_result.append(lrjust(\"%s\" % cell, 6))\n",
    "            \n",
    "        rows.append(\"|\".join(row_result))\n",
    "        \n",
    "    print (\"\\n\" + \"-\" * len(rows[0]) + \"\\n\").join(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cases\n",
    "\n",
    "In this simple test there's only one cell with a reward, so what we're really looking for is the shortest path to that cell.\n",
    "\n",
    "If a cell of the policy matrix contains several action, then this means that we can take any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward\n",
      "   0  |   0  |   0  |   0  \n",
      "---------------------------\n",
      "   0  |   0  |   1  |   0  \n",
      "---------------------------\n",
      "   0  |   0  |   0  |   0  \n",
      "\n",
      "Policy\n",
      "  →↓  |  →↓  |   ↓  |  ↓←  \n",
      "---------------------------\n",
      "   →  |   →  | →↑↓← |   ←  \n",
      "---------------------------\n",
      "  →↑  |  →↑  |   ↑  |  ↑←  \n",
      "\n",
      "Values\n",
      " 0.17 | 0.33 | 0.67 | 0.33 \n",
      "---------------------------\n",
      " 0.33 | 0.67 | 1.33 | 0.67 \n",
      "---------------------------\n",
      " 0.17 | 0.33 | 0.67 | 0.33 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "R = np.array([\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ])\n",
    "\n",
    "print \"Reward\"\n",
    "draw_matrix(R)\n",
    "print\n",
    "\n",
    "grid_world = GridWorld(R)\n",
    "agent = DPAgent(grid_world, discount_factor=0.5)\n",
    "\n",
    "for k in range(10):\n",
    "    agent.learn()\n",
    "    \n",
    "draw_policy(agent, 3, 4)\n",
    "draw_values(agent, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test case there's a single path to a reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward\n",
      "   0  |   0  |   0  |   0  \n",
      "---------------------------\n",
      "   0  |  -1  |  -1  |  -1  \n",
      "---------------------------\n",
      "   0  |   0  |   0  |   0  \n",
      "---------------------------\n",
      "  -1  |  -1  |  -1  |   0  \n",
      "---------------------------\n",
      "   1  |   0  |   0  |   0  \n",
      "\n",
      "Policy\n",
      "   ↓  |   ←  |   ←  |   ←  \n",
      "---------------------------\n",
      "   ↓  |   ↓  |   ↓  |   ↓  \n",
      "---------------------------\n",
      "   →  |   →  |   →  |   ↓  \n",
      "---------------------------\n",
      "   ↓  |   ↓  |   ↓  |   ↓  \n",
      "---------------------------\n",
      "   →  |   ←  |   ←  |   ←  \n",
      "\n",
      "Values\n",
      " 0.00 | 0.00 | 0.00 | 0.00 \n",
      "---------------------------\n",
      " 0.00 | -0.99| -0.99| -0.98\n",
      "---------------------------\n",
      " 0.01 | 0.01 | 0.02 | 0.04 \n",
      "---------------------------\n",
      " -0.33| -0.67| -0.83| 0.08 \n",
      "---------------------------\n",
      " 1.33 | 0.67 | 0.33 | 0.17 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "R2 = np.array([\n",
    "        [0, 0, 0, 0],\n",
    "        [0, -1, -1, -1],\n",
    "        [0, 0, 0, 0],\n",
    "        [-1, -1, -1, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ])\n",
    "\n",
    "print \"Reward\"\n",
    "draw_matrix(R2)\n",
    "print\n",
    "\n",
    "grid_world = GridWorld(R2)\n",
    "agent = DPAgent(grid_world, discount_factor=0.5)\n",
    "\n",
    "for k in range(50):\n",
    "    agent.learn()\n",
    "    \n",
    "draw_policy(agent, 5, 4)\n",
    "draw_values(agent, 5, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

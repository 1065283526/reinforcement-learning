{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observable environments\n",
    "\n",
    "In contrast to the DP chapter, we now work with environments where we don't know about all dynamics. Instead, we can interact with the environment and observe its reactions. This allows us to sample from the environment.\n",
    "\n",
    "Monte Carlo methods perform a lot of these sampling steps, and try to draw conclusions from the results. By the law of large numbers, averaging the returns from many samples converges to the true expected value.\n",
    "\n",
    "One important constraint is that the methods from this chapter only work in environments with terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ObservableEnvironment:\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Return the set of possible states\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\"\n",
    "        Returns the actions that can be taken from the given state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the new state and the given reward. This does not have to\n",
    "        be deterministic\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"\n",
    "        Returns a boolean indicating whether the state is terminal\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def sample(self, policy, state):\n",
    "        \"\"\"\n",
    "        Follows the policy until a terminal state is reached.\n",
    "        Returns a list of states, actions, and the rewards they gave\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        while not self.is_terminal_state(state):\n",
    "            actions = self.get_possible_actions(state)\n",
    "            ps = [policy[(state, action)] for action in actions]\n",
    "            \n",
    "            action = np.random.choice(actions, p=ps)\n",
    "            \n",
    "            new_state, reward = self.execute_action(state, action)\n",
    "            \n",
    "            result.append((state, action, reward))\n",
    "            state = new_state\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlackJack\n",
    "\n",
    "In this chapter, we try to find an optimal policy to play a simplified version of BlackJack. The game is described in detail in Example 5.1 of Sutton & Barto's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from random import choice\n",
    "\n",
    "class BlackJack(ObservableEnvironment):\n",
    "    def __init__(self):\n",
    "        self._init_deck()\n",
    "        \n",
    "        self.HIT = 0\n",
    "        self.STICK = 1\n",
    "        \n",
    "        self.WINNING_STATE = (21, 0, 0)\n",
    "        self.LOSING_STATE = (0, 21, 0)\n",
    "        self.DRAWING_STATE = (21, 21, 0)\n",
    "        self.terminal_states = [self.WINNING_STATE, self.LOSING_STATE, self.DRAWING_STATE]\n",
    "        \n",
    "        self.WON = (self.WINNING_STATE, 1)\n",
    "        self.LOST = (self.LOSING_STATE, -1)\n",
    "        self.DRAW = (self.DRAWING_STATE, 0)\n",
    "        \n",
    "    def _init_deck(self):\n",
    "        # Ace: 1\n",
    "        # Numbers: 2 to 10\n",
    "        # Jack/Queen/King: 10\n",
    "        self.deck = range(1, 10 + 1) + [10] * 3\n",
    "        \n",
    "    def get_states(self):\n",
    "        player_state = range(12, 21 + 1)\n",
    "        dealer_state = range(1, 10 + 1)\n",
    "        usable_ace = [0, 1]\n",
    "        \n",
    "        return list(product(player_state, dealer_state, usable_ace)) + self.terminal_states\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        player, dealer, usable_ace = state\n",
    "        return [self.HIT, self.STICK]\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        player, dealer, usable_ace = state\n",
    "        \n",
    "        if action == self.HIT:\n",
    "            new_card = self._sample_card()\n",
    "            player += new_card\n",
    "            \n",
    "            if new_card == 1:\n",
    "                usable_ace = 1\n",
    "            \n",
    "            if player > 21:\n",
    "                if usable_ace == 1:\n",
    "                    player -= 10\n",
    "                    usable_ace = 0\n",
    "                else:\n",
    "                    return self.LOST\n",
    "        elif action == self.STICK:\n",
    "            while dealer <= 17:\n",
    "                dealer += self._sample_card()\n",
    "                \n",
    "            if player < dealer <= 21:\n",
    "                return self.LOST\n",
    "            else:\n",
    "                return self.WON\n",
    "        \n",
    "        state = (player, dealer, usable_ace)\n",
    "        return state, 0\n",
    "    \n",
    "    def _sample_card(self):\n",
    "        return choice(range(1, 10 + 1))\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        return state in self.terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def print_policy(self):\n",
    "        policy = self.policy\n",
    "        \n",
    "        for state in self.env.get_states():\n",
    "            if self.env.is_terminal_state(state):\n",
    "                continue\n",
    "            \n",
    "            actions = self.env.get_possible_actions(state)                \n",
    "            QA = { action: self.Q[(state, action)] for action in actions }\n",
    "            best_action = max(QA.items(), key=itemgetter(1))[0]\n",
    "            \n",
    "            print state, best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "class OnPolicyFirstVisitMonteCarloAgent(Agent):\n",
    "    def __init__(self, env, epsilon=.05):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        self.policy = {}\n",
    "        \n",
    "        for state in self.env.get_states():\n",
    "            actions = self.env.get_possible_actions(state)\n",
    "            num_actions = len(actions)\n",
    "            \n",
    "            for action in actions:\n",
    "                self.Q[(state, action)] = 0\n",
    "                self.returns[(state, action)] = []\n",
    "                self.policy[(state, action)] = 1. / num_actions\n",
    "        \n",
    "    def learn(self, num_samples):\n",
    "        for _ in range(num_samples):\n",
    "            episode = self.env.sample(self.policy, self._get_start_state())\n",
    "            \n",
    "            if len(episode) == 0:\n",
    "                continue\n",
    "            \n",
    "            returns = {}\n",
    "            reward_after = 0\n",
    "            \n",
    "            for state, action, reward in reversed(episode):\n",
    "                reward_after += reward\n",
    "                returns[(state, action)] = reward_after\n",
    "                \n",
    "            for (state, action), ret in returns.items():\n",
    "                self.returns[(state, action)].append(ret)\n",
    "                self.Q[(state, action)] = mean(self.returns[(state, action)])\n",
    "            \n",
    "            states, _, _ = zip(*episode)\n",
    "            for state in set(states):\n",
    "                actions = self.env.get_possible_actions(state)\n",
    "                num_actions = len(actions)\n",
    "                \n",
    "                QA = { action: self.Q[(state, action)] for action in actions }\n",
    "                best_action = max(QA.items(), key=itemgetter(1))[0]\n",
    "                \n",
    "                for action in actions:\n",
    "                    self.policy[(state, action)] = self.epsilon / num_actions\n",
    "            \n",
    "                self.policy[(state, best_action)] += 1 - self.epsilon\n",
    "            \n",
    "    def _get_start_state(self):\n",
    "        return choice(self.env.get_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By learning from 500,000 samples we can learn the optimal policy. The same policy is shown in the book, but visualized a bit nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 0) 0\n",
      "(12, 1, 1) 0\n",
      "(12, 2, 0) 0\n",
      "(12, 2, 1) 0\n",
      "(12, 3, 0) 0\n",
      "(12, 3, 1) 0\n",
      "(12, 4, 0) 0\n",
      "(12, 4, 1) 0\n",
      "(12, 5, 0) 0\n",
      "(12, 5, 1) 0\n",
      "(12, 6, 0) 0\n",
      "(12, 6, 1) 0\n",
      "(12, 7, 0) 0\n",
      "(12, 7, 1) 0\n",
      "(12, 8, 0) 0\n",
      "(12, 8, 1) 0\n",
      "(12, 9, 0) 0\n",
      "(12, 9, 1) 0\n",
      "(12, 10, 0) 0\n",
      "(12, 10, 1) 0\n",
      "(13, 1, 0) 0\n",
      "(13, 1, 1) 0\n",
      "(13, 2, 0) 0\n",
      "(13, 2, 1) 0\n",
      "(13, 3, 0) 0\n",
      "(13, 3, 1) 0\n",
      "(13, 4, 0) 0\n",
      "(13, 4, 1) 0\n",
      "(13, 5, 0) 0\n",
      "(13, 5, 1) 0\n",
      "(13, 6, 0) 0\n",
      "(13, 6, 1) 0\n",
      "(13, 7, 0) 0\n",
      "(13, 7, 1) 0\n",
      "(13, 8, 0) 0\n",
      "(13, 8, 1) 0\n",
      "(13, 9, 0) 0\n",
      "(13, 9, 1) 0\n",
      "(13, 10, 0) 0\n",
      "(13, 10, 1) 0\n",
      "(14, 1, 0) 0\n",
      "(14, 1, 1) 0\n",
      "(14, 2, 0) 0\n",
      "(14, 2, 1) 0\n",
      "(14, 3, 0) 0\n",
      "(14, 3, 1) 0\n",
      "(14, 4, 0) 0\n",
      "(14, 4, 1) 0\n",
      "(14, 5, 0) 0\n",
      "(14, 5, 1) 0\n",
      "(14, 6, 0) 0\n",
      "(14, 6, 1) 0\n",
      "(14, 7, 0) 1\n",
      "(14, 7, 1) 0\n",
      "(14, 8, 0) 0\n",
      "(14, 8, 1) 0\n",
      "(14, 9, 0) 0\n",
      "(14, 9, 1) 0\n",
      "(14, 10, 0) 0\n",
      "(14, 10, 1) 0\n",
      "(15, 1, 0) 0\n",
      "(15, 1, 1) 0\n",
      "(15, 2, 0) 0\n",
      "(15, 2, 1) 0\n",
      "(15, 3, 0) 0\n",
      "(15, 3, 1) 0\n",
      "(15, 4, 0) 0\n",
      "(15, 4, 1) 0\n",
      "(15, 5, 0) 0\n",
      "(15, 5, 1) 0\n",
      "(15, 6, 0) 1\n",
      "(15, 6, 1) 0\n",
      "(15, 7, 0) 0\n",
      "(15, 7, 1) 0\n",
      "(15, 8, 0) 0\n",
      "(15, 8, 1) 0\n",
      "(15, 9, 0) 0\n",
      "(15, 9, 1) 0\n",
      "(15, 10, 0) 0\n",
      "(15, 10, 1) 0\n",
      "(16, 1, 0) 0\n",
      "(16, 1, 1) 0\n",
      "(16, 2, 0) 1\n",
      "(16, 2, 1) 0\n",
      "(16, 3, 0) 1\n",
      "(16, 3, 1) 0\n",
      "(16, 4, 0) 1\n",
      "(16, 4, 1) 0\n",
      "(16, 5, 0) 1\n",
      "(16, 5, 1) 0\n",
      "(16, 6, 0) 1\n",
      "(16, 6, 1) 0\n",
      "(16, 7, 0) 1\n",
      "(16, 7, 1) 0\n",
      "(16, 8, 0) 1\n",
      "(16, 8, 1) 0\n",
      "(16, 9, 0) 0\n",
      "(16, 9, 1) 0\n",
      "(16, 10, 0) 0\n",
      "(16, 10, 1) 0\n",
      "(17, 1, 0) 1\n",
      "(17, 1, 1) 0\n",
      "(17, 2, 0) 1\n",
      "(17, 2, 1) 0\n",
      "(17, 3, 0) 1\n",
      "(17, 3, 1) 0\n",
      "(17, 4, 0) 1\n",
      "(17, 4, 1) 0\n",
      "(17, 5, 0) 1\n",
      "(17, 5, 1) 0\n",
      "(17, 6, 0) 1\n",
      "(17, 6, 1) 0\n",
      "(17, 7, 0) 1\n",
      "(17, 7, 1) 0\n",
      "(17, 8, 0) 1\n",
      "(17, 8, 1) 0\n",
      "(17, 9, 0) 1\n",
      "(17, 9, 1) 0\n",
      "(17, 10, 0) 0\n",
      "(17, 10, 1) 0\n",
      "(18, 1, 0) 1\n",
      "(18, 1, 1) 0\n",
      "(18, 2, 0) 1\n",
      "(18, 2, 1) 1\n",
      "(18, 3, 0) 1\n",
      "(18, 3, 1) 1\n",
      "(18, 4, 0) 1\n",
      "(18, 4, 1) 0\n",
      "(18, 5, 0) 1\n",
      "(18, 5, 1) 1\n",
      "(18, 6, 0) 1\n",
      "(18, 6, 1) 1\n",
      "(18, 7, 0) 1\n",
      "(18, 7, 1) 0\n",
      "(18, 8, 0) 1\n",
      "(18, 8, 1) 1\n",
      "(18, 9, 0) 1\n",
      "(18, 9, 1) 1\n",
      "(18, 10, 0) 1\n",
      "(18, 10, 1) 0\n",
      "(19, 1, 0) 1\n",
      "(19, 1, 1) 1\n",
      "(19, 2, 0) 1\n",
      "(19, 2, 1) 1\n",
      "(19, 3, 0) 1\n",
      "(19, 3, 1) 1\n",
      "(19, 4, 0) 1\n",
      "(19, 4, 1) 1\n",
      "(19, 5, 0) 1\n",
      "(19, 5, 1) 1\n",
      "(19, 6, 0) 1\n",
      "(19, 6, 1) 1\n",
      "(19, 7, 0) 1\n",
      "(19, 7, 1) 1\n",
      "(19, 8, 0) 1\n",
      "(19, 8, 1) 1\n",
      "(19, 9, 0) 1\n",
      "(19, 9, 1) 1\n",
      "(19, 10, 0) 1\n",
      "(19, 10, 1) 1\n",
      "(20, 1, 0) 1\n",
      "(20, 1, 1) 1\n",
      "(20, 2, 0) 1\n",
      "(20, 2, 1) 1\n",
      "(20, 3, 0) 1\n",
      "(20, 3, 1) 1\n",
      "(20, 4, 0) 1\n",
      "(20, 4, 1) 1\n",
      "(20, 5, 0) 1\n",
      "(20, 5, 1) 1\n",
      "(20, 6, 0) 1\n",
      "(20, 6, 1) 1\n",
      "(20, 7, 0) 1\n",
      "(20, 7, 1) 1\n",
      "(20, 8, 0) 1\n",
      "(20, 8, 1) 1\n",
      "(20, 9, 0) 1\n",
      "(20, 9, 1) 1\n",
      "(20, 10, 0) 1\n",
      "(20, 10, 1) 1\n",
      "(21, 1, 0) 1\n",
      "(21, 1, 1) 1\n",
      "(21, 2, 0) 1\n",
      "(21, 2, 1) 1\n",
      "(21, 3, 0) 1\n",
      "(21, 3, 1) 1\n",
      "(21, 4, 0) 1\n",
      "(21, 4, 1) 1\n",
      "(21, 5, 0) 1\n",
      "(21, 5, 1) 1\n",
      "(21, 6, 0) 1\n",
      "(21, 6, 1) 1\n",
      "(21, 7, 0) 1\n",
      "(21, 7, 1) 1\n",
      "(21, 8, 0) 1\n",
      "(21, 8, 1) 1\n",
      "(21, 9, 0) 1\n",
      "(21, 9, 1) 1\n",
      "(21, 10, 0) 1\n",
      "(21, 10, 1) 1\n"
     ]
    }
   ],
   "source": [
    "env = BlackJack()\n",
    "agent = OnPolicyFirstVisitMonteCarloAgent(env)\n",
    "\n",
    "agent.learn(num_samples=500000)\n",
    "\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every-visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
